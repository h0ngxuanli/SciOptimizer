import nltk
from nltk.translate.bleu_score import sentence_bleu
from typing import List

# Dummy functions to generate text. Replace these with actual model invocation.
def generate_text_with_gpt(prompt: str) -> str:
    # Simulated GPT text generation
    return "This is a sample response from GPT based on the prompt."

def generate_text_with_other_model(prompt: str) -> str:
    # Simulated other model text generation
    return "This is a sample response from another model based on the prompt."

def compute_bleu(reference_texts: List[List[str]], candidate_text: str) -> float:
    """
    Compute the BLEU score between reference texts and a candidate text.
    
    Args:
    reference_texts (List[List[str]]): List of reference texts (each text split into tokens).
    candidate_text (str): Candidate text generated by the model (already tokenized).
    
    Returns:
    float: The BLEU score.
    """
    # Tokenize the candidate text
    candidate_tokens = candidate_text.split()
    
    # Compute the BLEU score
    score = sentence_bleu(reference_texts, candidate_tokens)
    return score

# Example usage
prompt = "Provide a brief introduction to BLEU score."
reference = ["The BLEU score is a measure used to evaluate the quality of text which has been machine translated from one language to another.".split()]

# Generate texts
gpt_text = generate_text_with_gpt(prompt)
other_model_text = generate_text_with_other_model(prompt)

# Compute BLEU Scores
gpt_bleu_score = compute_bleu(reference, gpt_text)
other_model_bleu_score = compute_bleu(reference, other_model_text)

print(f"BLEU Score for GPT: {gpt_bleu_score}")
print(f"BLEU Score for Other Model: {other_model_bleu_score}")